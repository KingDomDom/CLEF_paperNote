{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import esm\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from Bio import SeqIO\n",
    "from torch import einsum\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Args at 0x1d828e20910>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Args():\n",
    "    def __init__(self, In=None, Out=None, weight=None,cutoff=None):\n",
    "        self.In = In\n",
    "        self.Out = Out\n",
    "        self.weight = weight\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "args = Args(In = 'Test_clef_rep.pkl',\n",
    "            Out = 'Test_result.csv',\n",
    "            weight = './pretrained_model/T6classifier-CLEF-DP+MSA+3Di+AT-0.7cutoff.pt'  \n",
    ")\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_dnn(nn.Module):\n",
    "    def __init__(self, num_embeds = 1280, finial_drop = 0.5, out_dim = 1):\n",
    "        super().__init__()\n",
    "        self.dnn = nn.Sequential(nn.Linear(num_embeds, 2 * num_embeds), nn.ReLU(),\n",
    "                                 nn.Linear(2 * num_embeds, num_embeds))\n",
    "        self.out = nn.Sequential(nn.Linear(num_embeds, 128), nn.ReLU(),\n",
    "                                 nn.Linear(128, out_dim))\n",
    "        self.binaryclass = True if out_dim == 1 else False\n",
    "        self.Dropout = nn.Dropout(finial_drop)\n",
    "        self.ln = nn.LayerNorm(num_embeds)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = x['feature']\n",
    "        x = self.ln(self.dnn(x))\n",
    "        x = self.out(self.Dropout(x))    \n",
    "        if self.binaryclass:\n",
    "            x = torch.sigmoid(x).squeeze(-1)  \n",
    "        \n",
    "            \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hidden_layer_dimensions(data_dict):\n",
    "    hidden_layer_size = None\n",
    "    for key, value in data_dict.items():\n",
    "        if not isinstance(value, np.ndarray):\n",
    "            raise ValueError(f\"Value for key '{key}' is not a numpy array.\")\n",
    "\n",
    "        current_size = value.shape[-1]\n",
    "        if hidden_layer_size is None:\n",
    "            hidden_layer_size = current_size\n",
    "        elif hidden_layer_size != current_size:\n",
    "            return None  \n",
    "\n",
    "    return hidden_layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_feature_from_local(feature_path, silence=False):\n",
    "    '''\n",
    "    load feature dict from local path (Using pickle.load() or torch.load())\n",
    "    the dictionary is like:\n",
    "        {\n",
    "          Protein_ID : feature_array [a 1D numpy array]\n",
    "        }\n",
    "    '''\n",
    "    # Try pickle.load() function \n",
    "    try:\n",
    "        with open(feature_path, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "        if not silence:\n",
    "            print(\"File is loaded using pickle.load()\")\n",
    "        return obj\n",
    "    except (pickle.UnpicklingError, EOFError):\n",
    "        pass\n",
    "\n",
    "    # Try torch.load() function\n",
    "    try:\n",
    "        obj = torch.load(feature_path)\n",
    "        if not silence:\n",
    "            print(\"File is loaded using torch.load()\")\n",
    "        return obj\n",
    "    except (torch.serialization.UnsupportedPackageTypeError, RuntimeError):\n",
    "        pass\n",
    "\n",
    "    print(\"Unable to load file.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Potein_rep_datasets:\n",
    "  \n",
    "  def __init__(self, input_path, train_range = None, test_range = None, label_tag = 'label'):\n",
    "        '''\n",
    "        [input_path] is a Path_dict containing feature ID and corresponding Local_path\n",
    "        e.g {'feature':'./path/to/you/feature_file'}\n",
    "        '''\n",
    "        sequence_data = {}\n",
    "        try:\n",
    "            for key, value in input_path.items():\n",
    "                if isinstance(value, str):\n",
    "                    print(f\"try to load feature from path:{value}\")\n",
    "                    tmp = load_feature_from_local(value)\n",
    "                elif isinstance(value, np.ndarray):\n",
    "                    print(f\"try to load feature from numpy_array\")\n",
    "                    tmp = value\n",
    "                else:\n",
    "                    print(f\"can not load feature {key}\")\n",
    "                    continue\n",
    "                for ID, feat in tmp.items():\n",
    "                    if ID not in sequence_data:\n",
    "                        sequence_data[ID] = {key:feat}\n",
    "                    else:\n",
    "                        sequence_data[ID].update( {key:feat} )\n",
    "            \n",
    "            self.sequence_data = {}   \n",
    "            for key, value in sequence_data.items():\n",
    "                if len(value) < len(input_path):\n",
    "                   print(f\"imcomplete feature ID {key} removed\")\n",
    "                else:\n",
    "                   self.sequence_data[key] = value\n",
    "            \n",
    "            if label_tag not in input_path:\n",
    "                print(f\"Add mock label [{label_tag}] of 0 for each sample\")\n",
    "                for key in self.sequence_data:\n",
    "                    self.sequence_data[key][label_tag] = 0\n",
    "        except:\n",
    "            print(f\"No valid [input_path] to load : {input_path}, return an empty dataset\")\n",
    "            self.sequence_data = {}\n",
    "               \n",
    "        self.data_indices = {i : ID for i, ID in enumerate(self.sequence_data)}\n",
    "        \n",
    "        self.label_tag = label_tag\n",
    "        \n",
    "        print(f\"total {len(self.data_indices)} sample loaded\")\n",
    "        self.feature_list = list(input_path.keys())\n",
    "        \n",
    "        self.train_range = train_range\n",
    "        self.test_range = test_range\n",
    "        \n",
    "        if not self.train_range:\n",
    "            self.train_range = range(len(self.data_indices))\n",
    "      \n",
    "        if not self.test_range:\n",
    "            self.test_range = range(len(self.data_indices))\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "  def Dataloader(self, batch_size, shuffle = True, \n",
    "                          test = False,\n",
    "                          max_num_padding = None,\n",
    "                          device = 'cpu'):\n",
    "\n",
    "        sele_range = self.test_range if test else self.train_range\n",
    "        Nsample=len(list(sele_range))\n",
    "        indices=list(sele_range)\n",
    "        if shuffle:\n",
    "            random.shuffle(indices)\n",
    "        datasets = []\n",
    "        IDs = []\n",
    "        n = 0\n",
    "        for i in indices:\n",
    "            n += 1\n",
    "            \n",
    "            ID = self.data_indices[i]\n",
    "            IDs.append(ID)\n",
    "            data = self.sequence_data[ID]\n",
    "            datasets.append(data)\n",
    "            \n",
    "            if len(datasets) == batch_size or n == Nsample:\n",
    "                try:\n",
    "                    labels = torch.tensor([x[self.label_tag] for x in datasets]).to(torch.long)\n",
    "                except:\n",
    "                    print(f\"feature <{self.label_tag}> is not a valid label value, using mock labels instead.\")\n",
    "                    labels =  torch.tensor([0 for x in datasets]).to(torch.long)\n",
    "                batch = {\n",
    "                  'labels':labels.to(device),\n",
    "                  'ID':IDs\n",
    "                }\n",
    "                for key in self.feature_list:\n",
    "                    if key != self.label_tag:\n",
    "                      if max_num_padding:\n",
    "                          padded_seq_input = [pad_to_max_length(x[key], max_num_padding)[0] for x in datasets]\n",
    "                          valid_lens = torch.Tensor([pad_to_max_length(x[key], max_num_padding)[1] for x in datasets ]).to(torch.long)\n",
    "                          seq_input = np.concatenate([np.expand_dims(x, axis = 0) for x in padded_seq_input])\n",
    "                          seq_input = torch.from_numpy(seq_input).to(torch.float32)\n",
    "                          batch.update({key:seq_input.to(device)})\n",
    "                          if (valid_lens.max() > 0).item():\n",
    "                              if 'valid_lens' in batch :\n",
    "                                  try:\n",
    "                                      assert (batch['valid_lens'] == valid_lens).sum().item() == batch_size \n",
    "                                  except:\n",
    "                                      print(f\"Warning: please make sure valid lens of 2D tensors is the same \\n{batch['valid_lens']}\\n{valid_lens}\")\n",
    "                            \n",
    "                              batch.update({'valid_lens':valid_lens.to(device)})\n",
    "                      else:\n",
    "                          seq_input = np.vstack([x[key] for x in datasets])\n",
    "                          seq_input = torch.from_numpy(seq_input).to(torch.float32)\n",
    "                          batch.update({key:seq_input.to(device)})\n",
    "                \n",
    "                datasets = []\n",
    "                IDs = []\n",
    "                \n",
    "                yield batch\n",
    "    \n",
    "  def __len__(self):\n",
    "        return len(self.sequence_data)\n",
    "      \n",
    "  def split_test(self, test_size = 0.1):\n",
    "    \n",
    "        if len(self.sequence_data) > 1:\n",
    "            train_indices, test_indices = train_test_split(range(len(self.sequence_data)), test_size=test_size, random_state=42)\n",
    "            self.train_range = train_indices\n",
    "            self.test_range = test_indices\n",
    "        else:\n",
    "            print(f\"Number of {len(self)} data can not be splited.\")\n",
    "        \n",
    "  \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def pad_to_max_length(seq, max_length, len_dim = 0, feat_dim = 1):\n",
    "    if len(seq.shape) < 2:\n",
    "        return seq, 0\n",
    "    seq_length = seq.shape[len_dim]\n",
    "    if seq_length < max_length:\n",
    "        padded_seq = np.zeros([max_length, seq.shape[feat_dim]])\n",
    "        padded_seq[:seq_length] = seq\n",
    "        return padded_seq, seq_length\n",
    "    else:\n",
    "        return seq[:max_length, :], max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_from_1D_rep(input_file, initial_model, params_path,\n",
    "                        output_file = None,cutoff = None,\n",
    "                        Return = True):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    num_hidden = check_hidden_layer_dimensions(load_feature_from_local(input_file, silence=True))\n",
    "    assert num_hidden, f\"Dimension numbers of the last dimension is not same; {input_file}\"\n",
    "\n",
    "    model = initial_model(num_hidden).to(device)\n",
    "    eff_type = os.path.split(params_path)[-1].lower().split('classifier')[0].split('-')[-1]\n",
    "    eff_type = f'{eff_type.upper()}SE' if eff_type in ['t3', 't4', 't6'] else 'Effector'\n",
    "    Dataset = Potein_rep_datasets({'feature':input_file})\n",
    "    if not cutoff:\n",
    "        try:\n",
    "            cutoff = float(os.path.split(params_path)[-1].lower().split('cutoff')[0].split('-')[-1])\n",
    "        except:\n",
    "            cutoff = 0.5\n",
    "    print(f'Binary cutoff of {cutoff} used.')\n",
    "    output = {\n",
    "        'ID':[],\n",
    "        'pred':[],\n",
    "        eff_type:[]\n",
    "    }\n",
    "    model.load_state_dict(torch.load(params_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    Dataset.test_range = range(len(Dataset))\n",
    "    for batch in Dataset.Dataloader(batch_size=32,shuffle=False,max_num_padding=None,test=True,device=device):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(batch)\n",
    "        y_pred = y_pred.detach().to('cpu').numpy()\n",
    "        output['ID'].extend(batch['ID'])\n",
    "        output['pred'].append(y_pred)\n",
    "    output['pred'] = np.concatenate(output['pred'], 0)  \n",
    "    output['pred'] = list(output['pred'])  \n",
    "    output[eff_type] = ['Yes' if x >= cutoff else 'No' for x in output['pred']]\n",
    "    import pandas as pd\n",
    "    output = pd.DataFrame(output)\n",
    "    if output_file:\n",
    "        try:\n",
    "            output.to_csv(output_file)\n",
    "            print(f'Predictions saved as {output_file}')\n",
    "        except:\n",
    "            print(f'Predictions failed to save as {output_file}')\n",
    "            import uuid\n",
    "            tmp_name = str(uuid.uuid4())+'_clef'\n",
    "            tag = os.path.split(input_file)[-1]\n",
    "            output_file = os.path.join(os.path.dirname(input_file), f\"./{tag}_{eff_type}_prediction.csv\") \n",
    "            output.to_csv(output_file)\n",
    "            print(f'Predictions saved as {output_file}')    \n",
    "    if Return:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = args.In \n",
    "output_file = args.Out\n",
    "cutoff = args.cutoff\n",
    "classifier_path = args.weight   \n",
    "classifer = test_dnn\n",
    "config = {\n",
    "  'input_file':input_file,\n",
    "  'output_file':output_file,\n",
    "  'initial_model':classifer,\n",
    "  'params_path':classifier_path,\n",
    "  'cutoff':cutoff,\n",
    "  'Return':True\n",
    "}        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to load feature from path:Test_clef_rep.pkl\n",
      "File is loaded using pickle.load()\n",
      "Add mock label [label] of 0 for each sample\n",
      "total 10 sample loaded\n",
      "Binary cutoff of 0.7 used.\n",
      "Predictions saved as Test_result.csv\n",
      "               ID      pred T6SE\n",
      "0     NP_250554.1  0.993134  Yes\n",
      "1     NP_249515.1  0.999430  Yes\n",
      "2          4F0V_A  0.991892  Yes\n",
      "3     YP_898952.1  0.998459  Yes\n",
      "4      CBG37356.1  0.999968  Yes\n",
      "5  WP_151253718.1  0.000017   No\n",
      "6      ADZ63249.1  0.000073   No\n",
      "7          P46922  0.000089   No\n",
      "8          Q9TYU9  0.000034   No\n",
      "9          Q92F67  0.001061   No\n",
      "Predictions saved at Test_result.csv\n"
     ]
    }
   ],
   "source": [
    "output_dict = predict_from_1D_rep(**config)\n",
    "print(output_dict.iloc[:10,:])\n",
    "print(f\"Predictions saved at {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
