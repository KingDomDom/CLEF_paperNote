{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import esm\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from Bio import SeqIO\n",
    "from torch import einsum\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Args at 0x1d828fb1a80>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Args():\n",
    "    def __init__(self, In=None, Out=None, weight=None,cutoff=None):\n",
    "        self.In = In\n",
    "        self.Out = Out\n",
    "        self.weight = weight\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "args = Args(In = 'Test_clef_rep.pkl',\n",
    "            Out = 'Test_result.csv',\n",
    "            weight = './pretrained_model/T6classifier-CLEF-DP+MSA+3Di+AT-0.7cutoff.pt'  \n",
    ")\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_dnn(nn.Module):\n",
    "    def __init__(self, num_embeds = 1280, finial_drop = 0.5, out_dim = 1):\n",
    "        super().__init__()\n",
    "        self.dnn = nn.Sequential(nn.Linear(num_embeds, 2 * num_embeds), nn.ReLU(),\n",
    "                                 nn.Linear(2 * num_embeds, num_embeds))\n",
    "        self.out = nn.Sequential(nn.Linear(num_embeds, 128), nn.ReLU(),\n",
    "                                 nn.Linear(128, out_dim))\n",
    "        self.binaryclass = True if out_dim == 1 else False\n",
    "        self.Dropout = nn.Dropout(finial_drop)\n",
    "        self.ln = nn.LayerNorm(num_embeds)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = x['feature']\n",
    "        x = self.ln(self.dnn(x))\n",
    "        x = self.out(self.Dropout(x))    \n",
    "        if self.binaryclass:\n",
    "            x = torch.sigmoid(x).squeeze(-1)  \n",
    "        \n",
    "            \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hidden_layer_dimensions(data_dict):\n",
    "    hidden_layer_size = None\n",
    "    for key, value in data_dict.items():\n",
    "        if not isinstance(value, np.ndarray):\n",
    "            raise ValueError(f\"Value for key '{key}' is not a numpy array.\")\n",
    "\n",
    "        current_size = value.shape[-1]\n",
    "        if hidden_layer_size is None:\n",
    "            hidden_layer_size = current_size\n",
    "        elif hidden_layer_size != current_size:\n",
    "            return None  \n",
    "\n",
    "    return hidden_layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_feature_from_local(feature_path, silence=False):\n",
    "    '''\n",
    "    load feature dict from local path (Using pickle.load() or torch.load())\n",
    "    the dictionary is like:\n",
    "        {\n",
    "          Protein_ID : feature_array [a 1D numpy array]\n",
    "        }\n",
    "    '''\n",
    "    # Try pickle.load() function \n",
    "    try:\n",
    "        with open(feature_path, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "        if not silence:\n",
    "            print(\"File is loaded using pickle.load()\")\n",
    "        return obj\n",
    "    except (pickle.UnpicklingError, EOFError):\n",
    "        pass\n",
    "\n",
    "    # Try torch.load() function\n",
    "    try:\n",
    "        obj = torch.load(feature_path)\n",
    "        if not silence:\n",
    "            print(\"File is loaded using torch.load()\")\n",
    "        return obj\n",
    "    except (torch.serialization.UnsupportedPackageTypeError, RuntimeError):\n",
    "        pass\n",
    "\n",
    "    print(\"Unable to load file.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 69)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:69\u001b[1;36m\u001b[0m\n\u001b[1;33m    def Dataloader(self, batch_size, shuffle = True,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Potein_rep_datasets:\n",
    "  \n",
    "  def __init__(self, input_path, train_range = None, test_range = None, label_tag = 'label'):\n",
    "        '''\n",
    "        [input_path] is a Path_dict containing feature ID and corresponding Local_path\n",
    "        e.g {'feature':'./path/to/you/feature_file'}\n",
    "        '''\n",
    "        sequence_data = {}\n",
    "        try:\n",
    "            for key, value in input_path.items():\n",
    "                if isinstance(value, str):\n",
    "                    print(f\"try to load feature from path:{value}\")\n",
    "                    tmp = load_feature_from_local(value)\n",
    "                elif isinstance(value, np.ndarray):\n",
    "                    print(f\"try to load feature from numpy_array\")\n",
    "                    tmp = value\n",
    "                else:\n",
    "                    print(f\"can not load feature {key}\")\n",
    "                    continue\n",
    "                for ID, feat in tmp.items():\n",
    "                    if ID not in sequence_data:\n",
    "                        sequence_data[ID] = {key:feat}\n",
    "                    else:\n",
    "                        sequence_data[ID].update( {key:feat} )\n",
    "            \n",
    "            self.sequence_data = {}   \n",
    "            for key, value in sequence_data.items():\n",
    "                if len(value) < len(input_path):\n",
    "                   print(f\"imcomplete feature ID {key} removed\")\n",
    "                else:\n",
    "                   self.sequence_data[key] = value\n",
    "            \n",
    "            if label_tag not in input_path:\n",
    "                print(f\"Add mock label [{label_tag}] of 0 for each sample\")\n",
    "                for key in self.sequence_data:\n",
    "                    self.sequence_data[key][label_tag] = 0\n",
    "        except:\n",
    "            print(f\"No valid [input_path] to load : {input_path}, return an empty dataset\")\n",
    "            self.sequence_data = {}\n",
    "               \n",
    "        self.data_indices = {i : ID for i, ID in enumerate(self.sequence_data)}\n",
    "        \n",
    "        self.label_tag = label_tag\n",
    "        \n",
    "        print(f\"total {len(self.data_indices)} sample loaded\")\n",
    "        self.feature_list = list(input_path.keys())\n",
    "        \n",
    "        self.train_range = train_range\n",
    "        self.test_range = test_range\n",
    "        \n",
    "        if not self.train_range:\n",
    "            self.train_range = range(len(self.data_indices))\n",
    "      \n",
    "        if not self.test_range:\n",
    "            self.test_range = range(len(self.data_indices))\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "            return len(self.sequence_data)\n",
    "        \n",
    "    def split_test(self, test_size = 0.1):\n",
    "        \n",
    "            if len(self.sequence_data) > 1:\n",
    "                train_indices, test_indices = train_test_split(range(len(self.sequence_data)), test_size=test_size, random_state=42)\n",
    "                self.train_range = train_indices\n",
    "                self.test_range = test_indices\n",
    "            else:\n",
    "                print(f\"Number of {len(self)} data can not be splited.\")\n",
    "    def Dataloader(self, batch_size, shuffle = True, \n",
    "                            test = False,\n",
    "                            max_num_padding = None,\n",
    "                            device = 'cpu'):\n",
    "\n",
    "            sele_range = self.test_range if test else self.train_range\n",
    "            Nsample=len(list(sele_range))\n",
    "            indices=list(sele_range)\n",
    "            if shuffle:\n",
    "                random.shuffle(indices)\n",
    "            datasets = []\n",
    "            IDs = []\n",
    "            n = 0\n",
    "            for i in indices:\n",
    "                n += 1\n",
    "                \n",
    "                ID = self.data_indices[i]\n",
    "                IDs.append(ID)\n",
    "                data = self.sequence_data[ID]\n",
    "                datasets.append(data)\n",
    "                \n",
    "                if len(datasets) == batch_size or n == Nsample:\n",
    "                    try:\n",
    "                        labels = torch.tensor([x[self.label_tag] for x in datasets]).to(torch.long)\n",
    "                    except:\n",
    "                        print(f\"feature <{self.label_tag}> is not a valid label value, using mock labels instead.\")\n",
    "                        labels =  torch.tensor([0 for x in datasets]).to(torch.long)\n",
    "                    batch = {\n",
    "                    'labels':labels.to(device),\n",
    "                    'ID':IDs\n",
    "                    }\n",
    "                    for key in self.feature_list:\n",
    "                        if key != self.label_tag:\n",
    "                        if max_num_padding:\n",
    "                            padded_seq_input = [pad_to_max_length(x[key], max_num_padding)[0] for x in datasets]\n",
    "                            valid_lens = torch.Tensor([pad_to_max_length(x[key], max_num_padding)[1] for x in datasets ]).to(torch.long)\n",
    "                            seq_input = np.concatenate([np.expand_dims(x, axis = 0) for x in padded_seq_input])\n",
    "                            seq_input = torch.from_numpy(seq_input).to(torch.float32)\n",
    "                            batch.update({key:seq_input.to(device)})\n",
    "                            if (valid_lens.max() > 0).item():\n",
    "                                if 'valid_lens' in batch :\n",
    "                                    try:\n",
    "                                        assert (batch['valid_lens'] == valid_lens).sum().item() == batch_size \n",
    "                                    except:\n",
    "                                        print(f\"Warning: please make sure valid lens of 2D tensors is the same \\n{batch['valid_lens']}\\n{valid_lens}\")\n",
    "                                \n",
    "                                batch.update({'valid_lens':valid_lens.to(device)})\n",
    "                        else:\n",
    "                            seq_input = np.vstack([x[key] for x in datasets])\n",
    "                            seq_input = torch.from_numpy(seq_input).to(torch.float32)\n",
    "                            batch.update({key:seq_input.to(device)})\n",
    "                    \n",
    "                    datasets = []\n",
    "                    IDs = []\n",
    "                    \n",
    "                    yield batch\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_from_1D_rep(input_file, initial_model, params_path,\n",
    "                        output_file = None,cutoff = None,\n",
    "                        Return = True):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    num_hidden = check_hidden_layer_dimensions(load_feature_from_local(input_file, silence=True))\n",
    "    assert num_hidden, f\"Dimension numbers of the last dimension is not same; {input_file}\"\n",
    "\n",
    "    model = initial_model(num_hidden).to(device)\n",
    "    eff_type = os.path.split(params_path)[-1].lower().split('classifier')[0].split('-')[-1]\n",
    "    eff_type = f'{eff_type.upper()}SE' if eff_type in ['t3', 't4', 't6'] else 'Effector'\n",
    "    Dataset = Potein_rep_datasets({'feature':input_file})\n",
    "    if not cutoff:\n",
    "        try:\n",
    "            cutoff = float(os.path.split(params_path)[-1].lower().split('cutoff')[0].split('-')[-1])\n",
    "        except:\n",
    "            cutoff = 0.5\n",
    "    print(f'Binary cutoff of {cutoff} used.')\n",
    "    output = {\n",
    "        'ID':[],\n",
    "        'pred':[],\n",
    "        eff_type:[]\n",
    "    }\n",
    "    model.load_state_dict(torch.load(params_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    Dataset.test_range = range(len(Dataset))\n",
    "    for batch in Dataset.Dataloader(batch_size=32,shuffle=False,max_num_padding=None,test=True,device=device):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(batch)\n",
    "        y_pred = y_pred.detach().to('cpu').numpy()\n",
    "        output['ID'].extend(batch['ID'])\n",
    "        output['pred'].append(y_pred)\n",
    "    output['pred'] = np.concatenate(output['pred'], 0)  \n",
    "    output['pred'] = list(output['pred'])  \n",
    "    output[eff_type] = ['Yes' if x >= cutoff else 'No' for x in output['pred']]\n",
    "    import pandas as pd\n",
    "    output = pd.DataFrame(output)\n",
    "    if output_file:\n",
    "        try:\n",
    "            output.to_csv(output_file)\n",
    "            print(f'Predictions saved as {output_file}')\n",
    "        except:\n",
    "            print(f'Predictions failed to save as {output_file}')\n",
    "            import uuid\n",
    "            tmp_name = str(uuid.uuid4())+'_clef'\n",
    "            tag = os.path.split(input_file)[-1]\n",
    "            output_file = os.path.join(os.path.dirname(input_file), f\"./{tag}_{eff_type}_prediction.csv\") \n",
    "            output.to_csv(output_file)\n",
    "            print(f'Predictions saved as {output_file}')    \n",
    "    if Return:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = args.In \n",
    "output_file = args.Out\n",
    "cutoff = args.cutoff\n",
    "classifier_path = args.weight   \n",
    "classifer = test_dnn\n",
    "config = {\n",
    "  'input_file':input_file,\n",
    "  'output_file':output_file,\n",
    "  'initial_model':classifer,\n",
    "  'params_path':classifier_path,\n",
    "  'cutoff':cutoff,\n",
    "  'Return':True\n",
    "}        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to load feature from path:Test_clef_rep.pkl\n",
      "File is loaded using pickle.load()\n",
      "Add mock label [label] of 0 for each sample\n",
      "total 10 sample loaded\n",
      "Binary cutoff of 0.7 used.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Potein_rep_datasets' object has no attribute 'Dataloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m predict_from_1D_rep(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_dict\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m10\u001b[39m,:])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[44], line 26\u001b[0m, in \u001b[0;36mpredict_from_1D_rep\u001b[1;34m(input_file, initial_model, params_path, output_file, cutoff, Return)\u001b[0m\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     25\u001b[0m Dataset\u001b[38;5;241m.\u001b[39mtest_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Dataset))\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataloader\u001b[49m(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,max_num_padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     28\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m model(batch)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Potein_rep_datasets' object has no attribute 'Dataloader'"
     ]
    }
   ],
   "source": [
    "output_dict = predict_from_1D_rep(**config)\n",
    "print(output_dict.iloc[:10,:])\n",
    "print(f\"Predictions saved at {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
