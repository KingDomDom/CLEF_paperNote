{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import esm\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from Bio import SeqIO\n",
    "from torch import einsum\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Args at 0x262ec0f7400>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, mode=None, esm_model_path=None, weight=None, In=None, Out=None, Maxlen=None):\n",
    "        self.mode = mode\n",
    "        self.esm_model_path = esm_model_path\n",
    "        self.weight = weight\n",
    "        self.In = In\n",
    "        self.Out = Out\n",
    "        self.Maxlen = Maxlen\n",
    "\n",
    "args = Args(mode = 'clef',\n",
    "            esm_model_path = './pretrained_model/esm2_t33_650M-UR50D.pt',\n",
    "            weight = './pretrained_model/CLEF-DP+MSA+3Di+AT.pt',\n",
    "            In = 'Test_demo.faa',\n",
    "            Out = 'Test_clef_rep')\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_file': 'Test_demo.faa',\n",
       " 'output_file': 'Test_clef_rep',\n",
       " 'model_params_path': WindowsPath('f:/FDU/CLEF/Code/pretrained_model/CLEF-DP+MSA+3Di+AT.pt'),\n",
       " 'esm_config': {'pretrained_model_params': './pretrained_model/esm2_t33_650M-UR50D.pt'},\n",
       " 'maxlength': 256,\n",
       " 'mode': 'clef'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = args.mode\n",
    "esm_model_path = args.esm_model_path\n",
    "model_params_path = args.weight\n",
    "input_file = args.In\n",
    "output_file = args.Out\n",
    "maxlength = args.Maxlen\n",
    "\n",
    "#esm_model_path = Path(os.path.abspath(esm_model_path))\n",
    "esm_config = {'pretrained_model_params':esm_model_path} if esm_model_path else None\n",
    "\n",
    "model_params_path = Path(os.path.abspath(model_params_path))\n",
    "\n",
    "config = {\n",
    "    'input_file':input_file,\n",
    "    'output_file':output_file,\n",
    "    'model_params_path':model_params_path,\n",
    "    'esm_config':esm_config,\n",
    "    'maxlength':256,\n",
    "    'mode':mode\n",
    "    \n",
    "}\n",
    "\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_to_EsmRep(input_fasta, output_file = None, \n",
    "                      pretrained_model_params = None,\n",
    "                      maxlen = 256,\n",
    "                      Return = True, \n",
    "                      Final_pool = False):\n",
    "  '''\n",
    "  input_file : input local fasta file path \n",
    "  output_file : output encoded file path \n",
    "  '''\n",
    "  import torch\n",
    "  import esm\n",
    "  pretrained_model_params = pretrained_model_params if pretrained_model_params else os.path.join(find_root_path, 'Code/pretrained_model/esm2_t33_650M_UR50D.pt')\n",
    "  aa_dict = {amino_acid: i for i, amino_acid in enumerate(\"ACDEFGHIKLMNPQRSTVWYX\")}\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  try:\n",
    "      input_embedding_net, alphabet = esm.pretrained.load_model_and_alphabet_local(pretrained_model_params)\n",
    "  except:\n",
    "      print(f\"Skip loading local pre-trained ESM2 model from {pretrained_model_params}.\\nTry to use ESM2-650M downloaded from hub\")\n",
    "      weight_path = os.path.dirname(os.path.abspath(pretrained_model_params))\n",
    "      if os.path.exists(weight_path) and os.path.isdir(weight_path):\n",
    "            torch.hub.set_dir(weight_path)\n",
    "      else:\n",
    "            print(f\"Download ESM2-650M to ./cache\")\n",
    "      input_embedding_net, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  input_embedding_net = input_embedding_net.to(device)\n",
    "  input_embedding_net.eval()\n",
    "  output_dict = {}\n",
    "  real_maxlen = max(1, maxlen - 2)\n",
    "  num_layer = len(input_embedding_net.layers)\n",
    "  for record in SeqIO.parse(open(input_fasta), 'fasta'):\n",
    "    sequence = str(record.seq[: real_maxlen])  \n",
    "    sequence = \"\".join([x if x in aa_dict else 'X' for x in sequence])\n",
    "    data = [\n",
    "    (\"protein1\", sequence),\n",
    "      ]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    with torch.no_grad():\n",
    "      results = input_embedding_net(batch_tokens, repr_layers=[num_layer], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][num_layer]\n",
    "    embedding = np.array(token_representations.squeeze(0).detach().to('cpu')).astype(np.float16)\n",
    "    embedding = embedding[:real_maxlen + 2, ]\n",
    "    embedding = embedding.mean(0) if Final_pool else embedding\n",
    "    output_dict[record.id] = embedding\n",
    "  if output_file:\n",
    "      try:\n",
    "          with open(output_file, 'wb') as f:\n",
    "            pickle.dump(output_dict, f)\n",
    "          print(f'ESM2 array saved as {output_file}')\n",
    "      except:\n",
    "          print(f'ESM2 array failed to save as {output_file}')\n",
    "          import uuid\n",
    "          tmp_name = str(uuid.uuid4())+'_esm'\n",
    "          output_file =os.path.join(os.path.dirname(input_file), tmp_name) \n",
    "          with open(output_file, 'wb') as f:\n",
    "            pickle.dump(output_dict, f)\n",
    "          print(f'Temp ESM2 array saved as {output_file}')\n",
    "  if Return:\n",
    "      return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_fasta_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            first_line = file.readline().strip()\n",
    "            return first_line.startswith(\">\")\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root_path():\n",
    "    try:\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except:\n",
    "        current_dir = os.getcwd()\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "    return project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hidden_layer_dimensions(data_dict):\n",
    "    hidden_layer_size = None\n",
    "    for key, value in data_dict.items():\n",
    "        if not isinstance(value, np.ndarray):\n",
    "            raise ValueError(f\"Value for key '{key}' is not a numpy array.\")\n",
    "\n",
    "        current_size = value.shape[-1]\n",
    "        if hidden_layer_size is None:\n",
    "            hidden_layer_size = current_size\n",
    "        elif hidden_layer_size != current_size:\n",
    "            return None  \n",
    "\n",
    "    return hidden_layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_from_local(feature_path, silence=False):\n",
    "    '''\n",
    "    load feature dict from local path (Using pickle.load() or torch.load())\n",
    "    the dictionary is like:\n",
    "        {\n",
    "          Protein_ID : feature_array [a 1D numpy array]\n",
    "        }\n",
    "    '''\n",
    "    # Try pickle.load() function \n",
    "    try:\n",
    "        with open(feature_path, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "        if not silence:\n",
    "            print(\"File is loaded using pickle.load()\")\n",
    "        return obj\n",
    "    except (pickle.UnpicklingError, EOFError):\n",
    "        pass\n",
    "\n",
    "    # Try torch.load() function\n",
    "    try:\n",
    "        obj = torch.load(feature_path)\n",
    "        if not silence:\n",
    "            print(\"File is loaded using torch.load()\")\n",
    "        return obj\n",
    "    except (torch.serialization.UnsupportedPackageTypeError, RuntimeError):\n",
    "        pass\n",
    "\n",
    "    print(\"Unable to load file.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clef_feature(input_file, \n",
    "                          output_file,\n",
    "                          model,\n",
    "                          params_path = None,\n",
    "                          loader_config = {'batch_size':64, 'max_num_padding':256},\n",
    "                          esm_config = {'Final_pool':False, 'maxlen':256, 'Return':False},\n",
    "                          MLP_proj = False,\n",
    "                          res_rep = False,\n",
    "                          Return = False):\n",
    "    from Data_utils import Potein_rep_datasets\n",
    "    import torch\n",
    "    if is_fasta_file(input_file):\n",
    "        print(f\"Transform representation from fasta file {input_file}\")\n",
    "        import uuid\n",
    "        tmp_file = str(uuid.uuid4())+'_tmp'\n",
    "        tmp_file = os.path.join(os.path.dirname(input_file), tmp_file)\n",
    "        esm_config = esm_config if isinstance(esm_config, dict) else {'Final_pool':False, 'maxlen':256, 'Return':False}\n",
    "        esm_config['input_fasta'] = input_file\n",
    "        esm_config['output_file'] = tmp_file\n",
    "        esm_config['Return'] = False\n",
    "        try:\n",
    "            fasta_to_EsmRep(**esm_config)\n",
    "        except:\n",
    "            print(\"Failed to transform fasta into ESM embeddings, make sure esm config is correct\")\n",
    "        tmpset = Potein_rep_datasets({'esm_feature':tmp_file})\n",
    "        try:\n",
    "            os.remove(tmp_file)\n",
    "            print(\"Tmp esm file {tmp_file} removed.\")\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        print(f\"Direct load esm representations from {input_file}\")\n",
    "        tmpset = Potein_rep_datasets({'esm_feature':input_file})\n",
    "    \n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    if isinstance(model, torch.nn.Module):\n",
    "        model.eval()\n",
    "    if params_path:\n",
    "      print(f\"Try to load model weights from {params_path}\")\n",
    "      try:\n",
    "        loaded_params = torch.load(params_path, map_location=device)\n",
    "        model.load_state_dict(loaded_params)\n",
    "        print(f\"Load model weights successfully\")\n",
    "      except:\n",
    "        print(f\"Failed to load model weights from {params_path}\")\n",
    "    \n",
    "    loader_config = loader_config if isinstance(loader_config, dict) else {'batch_size':64, 'max_num_padding':256}\n",
    "    loader_config['shuffle'] = False\n",
    "    loader_config['device'] = device\n",
    "    IDs = []\n",
    "    features = []\n",
    "    for batch in tmpset.Dataloader(**loader_config):\n",
    "        with torch.no_grad():\n",
    "            feat, proj_feat = model(batch, Return_res_rep = res_rep)\n",
    "        feature = proj_feat if MLP_proj else feat\n",
    "        feature_list = [feature[i,:].detach().to('cpu').numpy() for i in range(feature.shape[0])]\n",
    "        IDs.extend(batch['ID'])\n",
    "        features.extend(feature_list)\n",
    "    output_dict = {ID:feat.astype(np.float16) for ID, feat in zip(IDs, features)}\n",
    "    \n",
    "    if output_file:\n",
    "        try:\n",
    "            with open(output_file, 'wb') as f:\n",
    "              pickle.dump(output_dict, f)\n",
    "            print(f'CLEF array saved as {output_file}')\n",
    "        except:\n",
    "            print(f'CLEF array failed to save as {output_file}')\n",
    "            import uuid\n",
    "            tmp_name = str(uuid.uuid4())+'_clef'\n",
    "            output_file =os.path.join(os.path.dirname(input_file), tmp_name) \n",
    "            with open(output_file, 'wb') as f:\n",
    "              pickle.dump(output_dict, f)\n",
    "            print(f'Temp CLEF array saved as {output_file}')\n",
    "    if Return:\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_key=64, dim_value=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.scale = dim_key ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(dim, dim_key * heads, bias=False)\n",
    "        self.to_k = nn.Linear(dim, dim_key * heads, bias=False)\n",
    "        self.to_v = nn.Linear(dim, dim_value * heads, bias=False)\n",
    "        self.to_out = nn.Linear(dim_value * heads, dim)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        # 初始化模型参数\n",
    "        self.reset_parameter()\n",
    "\n",
    "    def reset_parameter(self):\n",
    "        # xavier初始化使输入输出方差一致：xavier_uniform_均匀分布初始化，xavier_normal_正态分布初始化\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_k.weight)\n",
    "        nn.init.xavier_uniform_(self.to_v.weight)\n",
    "        nn.init.xavier_uniform_(self.to_out.weight)\n",
    "        nn.init.zeros_(self.to_out.bias)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x:[batchsize, sequence_length, dim]\n",
    "        n, h = x.shape[-2], self.heads\n",
    "\n",
    "        # 从 x 生成 q, k, v  [batchsize, sequence_length, dim_k * heads]\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "\n",
    "        # map （函数，可迭代对象）； lambda 变量：处理方式 ；\n",
    "        #  [batch_size, heads, sequence_length, dim_key]\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
    "\n",
    "        # q/dim**0.5\n",
    "        q = q * self.scale\n",
    "        \n",
    "        # q, k 计算点积注意力 [batchsize, head, sequence_length, sequence_length]\n",
    "        logits = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "\n",
    "        # -1e9掩码\n",
    "        if mask is not None:\n",
    "            logits.masked_fill(mask, -1e9)\n",
    "\n",
    "        # softmax(q*k/d**0.5) [batchsize, head, sequence_length, sequence_length]\n",
    "        attn = logits.softmax(dim=-1)\n",
    "        \n",
    "        # dropout\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        # v*softmax(q*k/d**0.5) [batchsize, head, sequence_length, dim_value]\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        \n",
    "        #  [batch_size, sequence_length,  dim_value * heads] \n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        #  dim_value * heads -> dim \n",
    "        return self.to_out(out), attn\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_dim, heads, dropout_rate, att_dropout=0.05):\n",
    "        super().__init__()\n",
    "        \n",
    "        # dim, head, qk_dim, v_dim, dropout， 隐藏层维度整除分类头\n",
    "        self.attn = Attention(hid_dim, heads, hid_dim //\n",
    "                              heads, hid_dim // heads, att_dropout)\n",
    "        \n",
    "        # feedforward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.LayerNorm(hid_dim),\n",
    "            nn.Linear(hid_dim, hid_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hid_dim * 2, hid_dim),\n",
    "            nn.Dropout(dropout_rate))\n",
    "        self.layernorm = nn.LayerNorm(hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        # [batch_size, sequence_length, hid_dim]\n",
    "        residual = x\n",
    "        x = self.layernorm(x)  # pre-LN\n",
    "        \n",
    "        # x = [batch_size, sequence_length,  hid_dim] \n",
    "        # attn = [batchsize, head, sequence_length, sequence_length]\n",
    "        x, attn = self.attn(x, mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.ffn(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sequence_mask(X, valid_lens):\n",
    "    mask = torch.zeros((X.shape[0], X.shape[1]), dtype = torch.bool).to(X.device)                  \n",
    "    expanded_valid_lens = valid_lens.view(-1, 1).expand(X.shape[0], X.shape[1])    \n",
    "    src_key_padding_mask = mask.masked_fill(torch.arange(X.shape[1]).to(X.device).view(1, -1).expand(X.shape[0], X.shape[1]) >= expanded_valid_lens, True)\n",
    "    return src_key_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder A\n",
    "class clef_enc(nn.Module): \n",
    "\n",
    "    def __init__(self, num_embeds, num_hiddens=128, finial_drop=0.1, mlp_relu=True):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerLayer(num_embeds, 8, 0.45, 0.05)\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.Dropout = nn.Dropout(finial_drop)\n",
    "        self.ln = nn.LayerNorm(num_embeds)\n",
    "        if mlp_relu:\n",
    "            # True则包含两个线性层和一个ReLU\n",
    "            self.mlp = nn.Sequential(nn.Linear(num_embeds, 2 * num_embeds), nn.ReLU(),\n",
    "                                     nn.Linear(2 * num_embeds, num_hiddens))\n",
    "        else:\n",
    "            # False则只包含一个线性层\n",
    "            self.mlp = nn.Linear(num_embeds, num_hiddens)\n",
    "\n",
    "\n",
    "    def forward(self, batch, Return_res_rep=False):\n",
    "\n",
    "        X, valid_lens = batch['esm_feature'], batch['valid_lens']\n",
    "\n",
    "        src_key_padding_mask = sequence_mask(X, valid_lens)\n",
    "        \n",
    "        # 前向传播过程，[b, n] -> [b, 1, 1, n]\n",
    "        for layer in self.layers:\n",
    "            X, _ = layer(X, mask=src_key_padding_mask.unsqueeze(1).unsqueeze(2))\n",
    "\n",
    "        # whether return embeddings per-residue\n",
    "        if not Return_res_rep:   \n",
    "            X = torch.cat([X[i, :valid_lens[i] + 2].mean(0).unsqueeze(0)\n",
    "                           for i in range(X.size(0))], dim=0)\n",
    "            proj_X = self.mlp(self.Dropout(X))\n",
    "        else:\n",
    "            proj_X = torch.cat([X[i, :valid_lens[i]].mean(0).unsqueeze(0)\n",
    "                                for i in range(X.size(0))], dim=0)\n",
    "            proj_X = self.mlp(self.Dropout(proj_X))\n",
    "\n",
    "        return X, proj_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ESM_feature(input_embeddings_path, output_file = \"temp\"):\n",
    "    \n",
    "    with open(input_embeddings_path, 'rb') as f:\n",
    "        input_embedding = pickle.load(f)\n",
    "    \n",
    "    output_dict = {}\n",
    "    \n",
    "    for key, value in input_embedding.items():\n",
    "\n",
    "        output_feat = value.mean(0)   \n",
    "        output = output_feat\n",
    "        output_dict[key] = output\n",
    "        \n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(output_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_protein_representation(input_file,\n",
    "                    output_file,\n",
    "                    model_params_path = None,\n",
    "                    tmp_dir = \"./tmp\",\n",
    "                    embedding_generator = fasta_to_EsmRep,\n",
    "                    esm_config = None,\n",
    "                    remove_tmp = True,\n",
    "                    mode = 'clef',\n",
    "                    maxlength = 256    # Hyperparameter determining how many amino acids are used in protein-encoding by PLM\n",
    "                    ):\n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.mkdir(tmp_dir)\n",
    "        print(f'Make a temp directory:{tmp_dir}')\n",
    "    if is_fasta_file(input_file):\n",
    "        print(f\"Transform representation from fasta file {input_file}\")\n",
    "        import uuid\n",
    "        tmp_file = str(uuid.uuid4())+'_tmp_esm'\n",
    "        tmp_file = os.path.join(tmp_dir, tmp_file)\n",
    "        esm_config = esm_config if isinstance(esm_config, dict) else {'Final_pool':False, 'maxlen':maxlength}\n",
    "        esm_config['input_fasta'] = input_file\n",
    "        esm_config['output_file'] = tmp_file\n",
    "        esm_config['Return'] = False\n",
    "        if 'pretrained_model_params' not in esm_config:\n",
    "            esm_config['pretrained_model_params'] = os.path.join(find_root_path(), \"./pretrained_model/esm2_t33_650M_UR50D.pt\")\n",
    "        try:\n",
    "            embedding_generator(**esm_config)\n",
    "        except:\n",
    "            print(\"Failed to transform fasta into ESM embeddings, make sure esm config is correct\")\n",
    "            import shutil\n",
    "            shutil.rmtree(tmp_dir)\n",
    "            sys.exit(1)\n",
    "\n",
    "       \n",
    "    if mode.lower() == 'clef':\n",
    "        print(f\"Using pre-trained encoder in CLEF to generate protein representations\")\n",
    "        num_hidden = check_hidden_layer_dimensions(load_feature_from_local(tmp_file, silence=True))\n",
    "        assert num_hidden, \"Dimension numbers of the last dimension is not same\"\n",
    "        device='cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        encoder = clef_enc(num_hidden).to(device)\n",
    "        try:\n",
    "            encoder.load_state_dict(torch.load(model_params_path, map_location=torch.device('cpu')), strict=False)\n",
    "            print(f\"Successfully load CLEF params from {model_params_path}.\")\n",
    "        except:\n",
    "            print(f\"Failed to load CLEF params from {model_params_path}, make sure it is a valid weights for CLEF\")\n",
    "            import shutil\n",
    "            shutil.rmtree(tmp_dir)\n",
    "            sys.exit(1)\n",
    "        tmp_output= output_file\n",
    "        loader_config = {'batch_size':64, 'max_num_padding':256}\n",
    "        config = {\n",
    "          'input_file':tmp_file,\n",
    "          'output_file':tmp_output,\n",
    "          'model':encoder,\n",
    "          'params_path':None,\n",
    "          'loader_config':loader_config\n",
    "        }\n",
    "        generate_clef_feature(**config)\n",
    "\n",
    "    elif mode.lower() == 'esm':\n",
    "        print(f\"Direct generate esm representations\")\n",
    "        conf = {\n",
    "        'input_embeddings_path' : tmp_file,\n",
    "        'output_file' : output_file,\n",
    "        }\n",
    "        generate_ESM_feature(**conf)\n",
    "        print(f\"ESM2 (protein) array saved as {output_file}\")\n",
    "    else:\n",
    "        print(f\"{mode} is not a valid mode tag, please select [clef] or [esm] for protein-reps generation\")\n",
    "        import shutil\n",
    "        shutil.rmtree(tmp_dir)\n",
    "        sys.exit(1)\n",
    "        \n",
    "    print(f\"Done..\")\n",
    "    \n",
    "    if remove_tmp:\n",
    "        import shutil \n",
    "        try:\n",
    "            shutil.rmtree(tmp_dir)\n",
    "            print(f\"Remove temp directory: {tmp_dir}.\")\n",
    "        except:\n",
    "            print(f\"Failed to remove temp file in {tmp_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform representation from fasta file Test_demo.faa\n",
      "Skip loading local pre-trained ESM2 model from ./pretrained_model/esm2_t33_650M-UR50D.pt.\n",
      "Try to use ESM2-650M downloaded from hub\n",
      "ESM2 array saved as ./tmp\\c1012338-b13d-47a4-b398-6ef20f3c630c_tmp_esm\n",
      "Using pre-trained encoder in CLEF to generate protein representations\n",
      "Successfully load CLEF params from f:\\FDU\\CLEF\\Code\\pretrained_model\\CLEF-DP+MSA+3Di+AT.pt.\n",
      "Direct load esm representations from ./tmp\\c1012338-b13d-47a4-b398-6ef20f3c630c_tmp_esm\n",
      "try to load feature from path:./tmp\\c1012338-b13d-47a4-b398-6ef20f3c630c_tmp_esm\n",
      "File is loaded using pickle.load()\n",
      "Add mock label [label] of 0 for each sample\n",
      "total 10 sample loaded\n",
      "CLEF array saved as Test_clef_rep\n",
      "Done..\n",
      "Remove temp directory: ./tmp.\n"
     ]
    }
   ],
   "source": [
    "generate_protein_representation(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_faa_file(faa_file_path):\n",
    "    \"\"\"\n",
    "    读取 .faa 文件并打印每个序列的 ID 和描述信息\n",
    "    \n",
    "    param faa_file_path: .faa 文件的路径\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 使用 SeqIO.parse 读取 .faa 文件\n",
    "        for record in SeqIO.parse(faa_file_path, \"fasta\"):\n",
    "            # 打印序列的 ID 和描述信息\n",
    "            print(f\"Sequence ID: {record.id}\")\n",
    "            print(f\"Description: {record.description}\")\n",
    "            print(f\"Sequence: {record.seq}\")\n",
    "            print(\"-\" * 50)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"文件未找到，请检查路径是否正确: {faa_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时发生错误: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ID: NP_250554.1\n",
      "Description: NP_250554.1 NP_250554.1 NC_002516:c2023166-2022411 [Pseudomonas aeruginosa PAO1]|T6SE\n",
      "Sequence: MTTRLPQLLLALLASAVSLAASADEVQVAVAANFTAPIQAIAKEFEKDTGHRLVAAYGATGQFYTQIKNGAPFQVFLSADDSTPAKLEQEGEVVPGSRFTYAIGTLALWSPKAGYVDAEGEVLKSGSFRHLSIANPKTAPYGLAATQAMDKLGLAATLGPKLVEGQNISQAYQFVSSGNAELGFVALSQIYKDGKVATGSAWIVPTELHDPIRQDAVILNKGKDNAAAKALVDYLKGAKAAALIKSYGYEL\n",
      "--------------------------------------------------\n",
      "Sequence ID: NP_249515.1\n",
      "Description: NP_249515.1 NP_249515.1 NC_002516:c899656-899138 [Pseudomonas aeruginosa PAO1]|T6SE\n",
      "Sequence: MSGKPAARVTDPTTCPVPGHGSNPIVQGSPDVVFDGLPAARQGDASACGSPMISAVSSTVLINGLPAVTLGSIGAHGNVVIGGSGTVLIGDVFTPAPRAPALPLNRNSVPCSGRFQLIDHETGKPVAGRRVRVWSSGGWNAFDTTDADGMTSWIERPTAEILYIDLVQRCDA\n",
      "--------------------------------------------------\n",
      "Sequence ID: 4F0V_A\n",
      "Description: 4F0V_A 4F0V_A Tse1; Chain A, Crystal Structure Of Type Effector Tse1 From Pseudomonas Aeruginousa [Pseudomonas fluorescens]|T6SE\n",
      "Sequence: MGSSHHHHHHSSGENLYFEGSHMASMTGGQQMGRMDSLDQCIVNACKNSWDKSYLAGTPNKDNCSGFVQSVAAELGVPMPRGNANAMVDGLEQSWTKLASGAEAAQKAAQGFLVIAGLKGRTYGHVAVVISGPLYRQKYPMCWCGSIAGAVGQSQGLKSVGQVWNRTDRDRLNYYVYSLASCSLPRAS\n",
      "--------------------------------------------------\n",
      "Sequence ID: YP_898952.1\n",
      "Description: YP_898952.1 YP_898952.1 NC_008601:c1397618-1396989 [Francisella tularensis subsp. novicida U112]|T6SE\n",
      "Sequence: MSEMITRQQVTSGETIHVRTDPTACIGSHPNCRLFIDSLTIAGEKLDKNIVAIEGGEDVTKADSATAAASVIRLSITPGSINPTISITLGVLIKSNVRTKIEEKVSSILQASATDMKIKLGNSNKKQEYKTDEAWGIMIDLSNLELYPISAKAFSISIEPTELMGVSKDGMSYHIISIDGLTTSQGSLPVCCAASTDKGVAKIGYIAAA\n",
      "--------------------------------------------------\n",
      "Sequence ID: CBG37356.1\n",
      "Description: CBG37356.1 CBG37356.1 NC_017626:4865605-4867287 [Escherichia coli 042]|T6SE\n",
      "Sequence: MTKYQGYDVTDATHKTSIHNDWKVVVAKKKPARGVTLTIGIFFDGTGNNRENTASRLMKFNECSAARQGVNQKDAQSCEDFLKEINKNSISNGSYRGYYSNIHWLNILYHPDQVLKKDQTSAQIKTYISGIGTAAGEADSVIGMGLGTSILDIFEGVVTKTDEAMERITQALSEFMGFNLSPDFCIAKIQFDVFGFSRGAAAARHFANRVMEQDPAIARAIAKGLRGDFYDGKPSGEVRFLGLFDTVAAIGGISNFFDINGRSNPGVKLELRPSVAKKVFQITAMNEYRYNFSLNSIKGMWPELALPGAHSDIGGGYNPVGSPLQENESLFLSCPEFEIVSDDTREMDTRVYRKAEQVRKMLMTLPALKHILPHGKLTTKIRSIGVNNSNQRRAGVIQKQVGAAVFFERMAVPNDWANVCLRVMLDAAQEAGVLFEPIRQTNTELQLPSELIFLADKAIAQGKAVRLGQEPQAFTEEELYIIGKYTHCSANWNIESDGNLWVDPTTGEIFIH\n",
      "--------------------------------------------------\n",
      "Sequence ID: WP_151253718.1\n",
      "Description: WP_151253718.1 pathogenicity island 2 effector protein SseE [Salmonella enterica]|T3SE\n",
      "Sequence: MVQEIEQWLRRHQVFTEPAYLGETAILLGQQFILSPYLVIYRIEAKEMIICEFRRLTPGQPRPQQLFHLLGLLRGIFVHHPQLTCLKMLIITDVLDEKKAMLRRKLLRILTVMGATFTQLDGDNWTVLSAEHLIQRRF\n",
      "--------------------------------------------------\n",
      "Sequence ID: ADZ63249.1\n",
      "Description: ADZ63249.1 ADZ63249.1 Nisin; Lantibiotic nisin-A [Lactococcus lactis]|T1SE\n",
      "Sequence: MSTKDFNLDLVSVSKKDSGASPRITSISLCTPGCKTGALMGCNMKTATCHCSIHVSK\n",
      "--------------------------------------------------\n",
      "Sequence ID: P46922\n",
      "Description: P46922 P46922|POSITIVE|LIPO|4 sp|P46922|OPUAC_BACSU Glycine betaine-binding protein OpuAC OS=Bacillus subtilis (strain 168) OX=224308 GN=opuAC PE=1 SV=1\n",
      "Sequence: MLKKIIGIGVSAMLALSLAACGSENDENASAAEQVNKTIIGIDPGSGIMSLTDKAMKDYDLNDWTLISASSAAMTATLKKSYDRKKPIIITGWTPHWMFSRYKLKYLDDPKQSYGSAEEIHTITRKGFSKEQPNAAKLLSQFKWTQDEMGEIMIKVEEGEKPAKVAAEYVNKHKDQIAEWTKGVQKVKGDKINLAYVAWDSEIASTNVIGKVLEDLGYEVTLTQVEAGPMWTAIATGSADASLSAWLPNTHKAYAAKYKGKYDDIGTSMTGVKMGLVVPQYMKNVNSIEDLKK\n",
      "--------------------------------------------------\n",
      "Sequence ID: Q9TYU9\n",
      "Description: Q9TYU9 Q9TYU9|EUKARYA|NO_SP|0 sp|Q9TYU9|EXC6_CAEEL Excretory canal abnormal protein 6 OS=Caenorhabditis elegans OX=6239 GN=exc-6 PE=1 SV=2\n",
      "Sequence: MTSDTIRQTLDELLLDKNGGSSNEARAFFLSQIIDQLKLISSQTDAERQLQKLQLKDPNDNIVKATPPPPPPPPPLISILQQAPPPPPPPPPPTLKAPPPPPILGLKTPSKSLKTPTPRPKECPTSFLPKKEKKTKTRTVQWSKINASVVQDDSVWGKLAKASNVDIDFDLLDNFFGIESLAVSGAAEVVKKSTRKDAHVELLTAKRSQNVAIMLKQFKNIDELIDDVSQNKPVAEIDALQNLFGMLPQSEEEEALRRYTGDISLLSPPSSFFYRLVQIQFYRLRIETQIFLSDFSRLMRELAPNVEILIRTSQEILTSPTLPRLLLIFVNMGNYLNGNNSQGNAFGFTLNSLWKLIDLKGNKQEFSLLHLLVTCEPDLVAHLQEELSTLKDASQISFDEIKISLKTLRDGRCKLEKQLETCSGASFTQFLELIKIDCKFELDEFGANYDKLTELQYQLADYFCENRNTFQLDECLKIFNFLMNRLQQTLKEHVTRETRKLKKEEKKETQTT\n",
      "--------------------------------------------------\n",
      "Sequence ID: Q92F67\n",
      "Description: Q92F67 Q92F67|POSITIVE|LIPO|4 sp|Q92F67|Y239_LISIN Uncharacterized lipoprotein Lin0239 OS=Listeria innocua serovar 6a (strain ATCC BAA-680 / CLIP 11262) OX=272626 GN=lin0239 PE=3 SV=1\n",
      "Sequence: MKLLKKGTTVLFVMIMAVMLVACGDKEESKTFSLSQNGVDSKLTYTYKGDKVTKQTAENTMLYTSMGIKTKEEAEKMLKETSEKFQNIEGLKESIEYKDDKAIETLEVDYTKISSEDLKKLPGMASTGDVSKGISMKESEKMLKSQGFKEVEK\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "faa_file_path = \"Test_demo.faa\"\n",
    "\n",
    "read_faa_file(faa_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clef_representations(file_path):\n",
    "    \"\"\"\n",
    "    从指定路径加载 CLEF 表示文件(pickle 格式)并返回一个 Pandas DataFrame\n",
    "    \n",
    "    param file_path: CLEF 表示文件的路径\n",
    "    return: 包含蛋白质表示的 Pandas DataFrame\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"文件未找到，请检查路径是否正确: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 使用 pickle 加载文件\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        # 将数据转换为 Pandas DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"成功加载文件: {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时发生错误: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载文件: Test_clef_rep.pkl\n",
      "      NP_250554.1  NP_249515.1    4F0V_A  YP_898952.1  CBG37356.1  \\\n",
      "0        0.461426    -2.650391 -1.516602     0.036682   -1.694336   \n",
      "1        1.628906     0.894043 -0.152954    -0.503418   -1.369141   \n",
      "2        1.270508    -1.605469 -0.790039    -0.363770    0.463135   \n",
      "3       -0.419434     0.491455  1.163086     0.724609    2.080078   \n",
      "4       -2.646484    -0.020370 -0.164795    -0.841309   -0.457764   \n",
      "...           ...          ...       ...          ...         ...   \n",
      "1275     1.754883    -0.254395 -0.066284     0.112549   -0.114441   \n",
      "1276     2.541016    -0.245239 -0.409424     0.169678    0.155518   \n",
      "1277     0.095154    -1.577148 -1.087891    -0.794434   -0.750000   \n",
      "1278     0.327637    -0.099121 -0.260498    -1.374023   -0.250488   \n",
      "1279     1.052734    -0.165283 -0.069275    -0.048523   -0.764648   \n",
      "\n",
      "      WP_151253718.1  ADZ63249.1    P46922    Q9TYU9    Q92F67  \n",
      "0           0.158569   -1.480469  1.726562 -0.763672  1.833008  \n",
      "1          -0.518066    0.584473  1.098633  1.202148  1.731445  \n",
      "2          -0.128906   -0.380615  1.492188 -0.735352  1.618164  \n",
      "3          -0.627441    0.946289  1.155273  1.010742  1.380859  \n",
      "4           0.346436    0.176025 -2.414062  0.466797 -2.203125  \n",
      "...              ...         ...       ...       ...       ...  \n",
      "1275        0.277832   -0.601074  0.459717 -0.968262  1.572266  \n",
      "1276        0.310059   -0.472168  1.806641  0.035461  1.508789  \n",
      "1277       -1.346680   -1.858398 -0.431396 -1.270508 -1.884766  \n",
      "1278       -1.346680    0.604492 -0.263184 -1.062500 -1.549805  \n",
      "1279       -0.058533    1.064453 -1.358398  0.612793 -1.044922  \n",
      "\n",
      "[1280 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Test_clef_rep.pkl\"\n",
    "df_clef_rep = load_clef_representations(file_path)\n",
    "\n",
    "if df_clef_rep is not None:\n",
    "    print(df_clef_rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
