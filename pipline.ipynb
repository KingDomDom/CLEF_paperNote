{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import esm\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from Bio import SeqIO\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x000001D2E74EF1F0>\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, Seq=None, Feat=None, Out=None, esm_model_path=None, lr=None, btz=None,epoch=None,maxlen=None):\n",
    "        self.Seq = Seq\n",
    "        self.Feat = Feat\n",
    "        self.Out = Out\n",
    "        self.esm_model_path = esm_model_path\n",
    "        self.lr = lr\n",
    "        self.btz = btz\n",
    "        self.epoch = epoch\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "args = Args(Seq = \"./Demo_train/Demo_trainset.faa\",\n",
    "            Feat = [\"./Demo_train/Demo_trainset_featA\", \"./Demo_train/Demo_trainset_featB\", \"./Demo_train/Demo_trainset_featC\"],\n",
    "            Out = \"Demo_clef\",\n",
    "            esm_model_path = \"./pretrained_model/esm2_t33_650M_UR50D.pt\",\n",
    "            lr = 0.00002,\n",
    "            epoch = 20,\n",
    "            btz = 128,\n",
    "            maxlen = 256)\n",
    "\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_file_config': {'seq': './Demo_train/Demo_trainset.faa', 'modal_0': './Demo_train/Demo_trainset_featA', 'modal_1': './Demo_train/Demo_trainset_featB', 'modal_2': './Demo_train/Demo_trainset_featC'}, 'output_dir': 'Demo_clef', 'train_config': {'lr': 2e-05, 'batch_size': 128, 'num_epoch': 20, 'maxlen': 256}, 'esm_config': {'maxlen': 256, 'pretrained_model_params': './pretrained_model/esm2_t33_650M_UR50D.pt'}}\n"
     ]
    }
   ],
   "source": [
    "seq_path = args.Seq\n",
    "modal_paths = args.Feat\n",
    "modal_path_dict = {f'modal_{i}':path for i,path in enumerate(modal_paths)}\n",
    "input_file_config = {'seq':seq_path}\n",
    "input_file_config.update(modal_path_dict)\n",
    "train_config = {'lr':args.lr, 'batch_size':args.btz, 'num_epoch':args.epoch, 'maxlen':args.maxlen}\n",
    "output_dir = args.Out\n",
    "esm_config = {'maxlen':args.maxlen}\n",
    "if args.esm_model_path:\n",
    "    esm_config['pretrained_model_params'] = args.esm_model_path\n",
    "config = {\n",
    "    'input_file_config':input_file_config,\n",
    "    'output_dir':output_dir,\n",
    "    'train_config':train_config,\n",
    "    'esm_config':esm_config\n",
    "}\n",
    "#train_clef(**config)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_key=64, dim_value=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.scale = dim_key ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(dim, dim_key * heads, bias=False)\n",
    "        self.to_k = nn.Linear(dim, dim_key * heads, bias=False)\n",
    "        self.to_v = nn.Linear(dim, dim_value * heads, bias=False)\n",
    "        self.to_out = nn.Linear(dim_value * heads, dim)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        # 初始化模型参数\n",
    "        self.reset_parameter()\n",
    "\n",
    "    def reset_parameter(self):\n",
    "        # xavier初始化使输入输出方差一致：xavier_uniform_均匀分布初始化，xavier_normal_正态分布初始化\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_k.weight)\n",
    "        nn.init.xavier_uniform_(self.to_v.weight)\n",
    "        nn.init.xavier_uniform_(self.to_out.weight)\n",
    "        nn.init.zeros_(self.to_out.bias)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x:[batchsize, sequence_length, dim]\n",
    "        n, h = x.shape[-2], self.heads\n",
    "\n",
    "        # 从 x 生成 q, k, v  [batchsize, sequence_length, dim_k * heads]\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "\n",
    "        # map （函数，可迭代对象）； lambda 变量：处理方式 ；\n",
    "        #  [batch_size, heads, sequence_length, dim_key]\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
    "\n",
    "        # q/dim**0.5\n",
    "        q = q * self.scale\n",
    "        \n",
    "        # q, k 计算点积注意力 [batchsize, head, sequence_length, sequence_length]\n",
    "        logits = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "\n",
    "        # -1e9掩码\n",
    "        if mask is not None:\n",
    "            logits.masked_fill(mask, -1e9)\n",
    "\n",
    "        # softmax(q*k/d**0.5) [batchsize, head, sequence_length, sequence_length]\n",
    "        attn = logits.softmax(dim=-1)\n",
    "        \n",
    "        # dropout\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        # v*softmax(q*k/d**0.5) [batchsize, head, sequence_length, dim_value]\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        \n",
    "        #  [batch_size, sequence_length,  dim_value * heads] \n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        #  dim_value * heads -> dim \n",
    "        return self.to_out(out), attn\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_dim, heads, dropout_rate, att_dropout=0.05):\n",
    "        super().__init__()\n",
    "        \n",
    "        # dim, head, qk_dim, v_dim, dropout， 隐藏层维度整除分类头\n",
    "        self.attn = Attention(hid_dim, heads, hid_dim //\n",
    "                              heads, hid_dim // heads, att_dropout)\n",
    "        \n",
    "        # feedforward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.LayerNorm(hid_dim),\n",
    "            nn.Linear(hid_dim, hid_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hid_dim * 2, hid_dim),\n",
    "            nn.Dropout(dropout_rate))\n",
    "        self.layernorm = nn.LayerNorm(hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        # [batch_size, sequence_length, hid_dim]\n",
    "        residual = x\n",
    "        x = self.layernorm(x)  # pre-LN\n",
    "        \n",
    "        # x = [batch_size, sequence_length,  hid_dim] \n",
    "        # attn = [batchsize, head, sequence_length, sequence_length]\n",
    "        x, attn = self.attn(x, mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.ffn(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_lens):\n",
    "    # [batchsize, sequence_length],生成初始mask并默认False\n",
    "    mask = torch.zeros((X.shape[0], X.shape[1]), dtype = torch.bool).to(X.device) \n",
    "    \n",
    "    # [batchsize, 1] -> [batchsize, sequence_length]，扩展有效长度并与mask一致                 \n",
    "    expanded_valid_lens = valid_lens.view(-1, 1).expand(X.shape[0], X.shape[1])    \n",
    "    \n",
    "    # 将超出expanded_valid_lens的部分记为True\n",
    "    src_key_padding_mask = mask.masked_fill(torch.arange(X.shape[1]).to(X.device).view(1, -1).expand(X.shape[0], X.shape[1]) >= expanded_valid_lens, True)\n",
    "    return src_key_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(feature):\n",
    "    mean = feature.mean(dim=0, keepdim=True)\n",
    "    std = feature.std(dim=0, keepdim=True)\n",
    "    return (feature - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder A\n",
    "class clef_enc(nn.Module): \n",
    "\n",
    "    def __init__(self, num_embeds, num_hiddens=128, finial_drop=0.1, mlp_relu=True):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerLayer(num_embeds, 8, 0.45, 0.05)\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.Dropout = nn.Dropout(finial_drop)\n",
    "        self.ln = nn.LayerNorm(num_embeds)\n",
    "        if mlp_relu:\n",
    "            # True则包含两个线性层和一个ReLU\n",
    "            self.mlp = nn.Sequential(nn.Linear(num_embeds, 2 * num_embeds), nn.ReLU(),\n",
    "                                     nn.Linear(2 * num_embeds, num_hiddens))\n",
    "        else:\n",
    "            # False则只包含一个线性层\n",
    "            self.mlp = nn.Linear(num_embeds, num_hiddens)\n",
    "\n",
    "\n",
    "    def forward(self, batch, Return_res_rep=False):\n",
    "\n",
    "        X, valid_lens = batch['esm_feature'], batch['valid_lens']\n",
    "\n",
    "        src_key_padding_mask = sequence_mask(X, valid_lens)\n",
    "        \n",
    "        # 前向传播过程，[b, n] -> [b, 1, 1, n]\n",
    "        for layer in self.layers:\n",
    "            X, _ = layer(X, mask=src_key_padding_mask.unsqueeze(1).unsqueeze(2))\n",
    "\n",
    "        # whether return embeddings per-residue\n",
    "        if not Return_res_rep:   \n",
    "            X = torch.cat([X[i, :valid_lens[i] + 2].mean(0).unsqueeze(0)\n",
    "                           for i in range(X.size(0))], dim=0)\n",
    "            proj_X = self.mlp(self.Dropout(X))\n",
    "        else:\n",
    "            proj_X = torch.cat([X[i, :valid_lens[i]].mean(0).unsqueeze(0)\n",
    "                                for i in range(X.size(0))], dim=0)\n",
    "            proj_X = self.mlp(self.Dropout(proj_X))\n",
    "\n",
    "        return X, proj_X\n",
    "\n",
    "\n",
    "\n",
    "class clef_multimodal(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeds, feat_dim_config, num_hiddens=128,\n",
    "                 finial_drop=0.1, mlp_relu=True, feat_mlp_relu=True,\n",
    "                 feature_norm=True):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerLayer(num_embeds, 8, 0.45, 0.05)\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.Dropout = nn.Dropout(finial_drop)\n",
    "        self.ln = nn.LayerNorm(num_embeds)\n",
    "        if mlp_relu:\n",
    "            self.mlp = nn.Sequential(nn.Linear(num_embeds, 2 * num_embeds), nn.ReLU(),\n",
    "                                     nn.Linear(2 * num_embeds, num_hiddens))\n",
    "        else:\n",
    "            self.mlp = nn.Linear(num_embeds, num_hiddens)\n",
    "            \n",
    "            \n",
    "        # 定义多模态字典\n",
    "        self.modal_indeces = {key:i for i, key in enumerate(feat_dim_config)}\n",
    "\n",
    "        # 将各个特征维度的值加和，之后通过线性层变为1024维度\n",
    "        feat_dim = sum([dim for dim in feat_dim_config.values()])\n",
    "        self.feat_encoder = nn.Sequential(nn.Linear(feat_dim, 1024, bias=False), nn.ReLU(),\n",
    "                              nn.Linear(1024, num_hiddens))\n",
    "\n",
    "\n",
    "\n",
    "        # 再对每个维度进行layernorm\n",
    "        self.feature_norm = feature_norm\n",
    "        if self.feature_norm:\n",
    "            self.ln_f = nn.LayerNorm(num_hiddens)\n",
    "\n",
    "    def forward(self, batch, Return_res_rep=False):\n",
    "\n",
    "        # 多个模态信息\n",
    "        has_modal = min([x in batch for x in self.modal_indeces.keys()])\n",
    "        \n",
    "        X, valid_lens = batch['esm_feature'], batch['valid_lens']\n",
    "\n",
    "        src_key_padding_mask = sequence_mask(X, valid_lens)\n",
    "        for layer in self.layers:\n",
    "            X, _ = layer(X, mask=src_key_padding_mask.unsqueeze(1).unsqueeze(2))\n",
    "\n",
    "        if not Return_res_rep:   # If return embeddings per-residue\n",
    "            X = torch.cat([X[i, :valid_lens[i] + 2].mean(0).unsqueeze(0)\n",
    "                           for i in range(X.size(0))], dim=0)\n",
    "            proj_X = self.mlp(self.Dropout(X))\n",
    "        else:\n",
    "            proj_X = torch.cat([X[i, :valid_lens[i]].mean(0).unsqueeze(0)\n",
    "                                for i in range(X.size(0))], dim=0)\n",
    "            proj_X = self.mlp(self.Dropout(proj_X))\n",
    "\n",
    "        # 如果含有多模态信息\n",
    "        if has_modal:\n",
    "            cross_features = []\n",
    "            for modal, i in self.modal_indeces.items():\n",
    "                if modal in batch:\n",
    "                    # 归一化\n",
    "                    norm_feat = standardize(batch[modal]) if len(self.modal_indeces.items())>1 else batch[modal]\n",
    "                    cross_features.append(norm_feat)\n",
    "            # 特征拼接在一起\n",
    "            cross_features = torch.cat(cross_features, -1)\n",
    "            \n",
    "            # 编码融合后的特征\n",
    "            proj_features = self.feat_encoder(cross_features)\n",
    "            if self.feature_norm:\n",
    "                proj_features = self.ln_f(proj_features)\n",
    "\n",
    "            # X是transformer的输出，proj_x经过mlp处理，proj_features融合后的多模态\n",
    "            return X, proj_X, proj_features\n",
    "        else:\n",
    "            return X, proj_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_to_EsmRep(input_fasta, output_file = None, \n",
    "                      pretrained_model_params = None,\n",
    "                      maxlen = 256,\n",
    "                      Return = True, \n",
    "                      Final_pool = False):\n",
    "  '''\n",
    "  input_file : input local fasta file path \n",
    "  output_file : output encoded file path \n",
    "  '''\n",
    "  pretrained_model_params = pretrained_model_params if pretrained_model_params else './pretrained_model/esm2_t33_650M_UR50D.pt'\n",
    "  aa_dict = {amino_acid: i for i, amino_acid in enumerate(\"ACDEFGHIKLMNPQRSTVWYX\")}\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  try:\n",
    "      input_embedding_net, alphabet = esm.pretrained.load_model_and_alphabet_local(pretrained_model_params)\n",
    "  except:\n",
    "      print(f\"Skip loading local pre-trained ESM2 model from {pretrained_model_params}.\\nTry to use ESM2-650M downloaded from hub\")\n",
    "      weight_path = os.path.dirname(os.path.abspath(pretrained_model_params))\n",
    "      if os.path.exists(weight_path) and os.path.isdir(weight_path):\n",
    "            torch.hub.set_dir(weight_path)\n",
    "      else:\n",
    "            print(f\"Download ESM2-650M to ./cache\")\n",
    "      input_embedding_net, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  input_embedding_net = input_embedding_net.to(device)\n",
    "  input_embedding_net.eval()\n",
    "  output_dict = {}\n",
    "  real_maxlen = max(1, maxlen - 2)\n",
    "  num_layer = len(input_embedding_net.layers)\n",
    "  for record in SeqIO.parse(open(input_fasta), 'fasta'):\n",
    "    sequence = str(record.seq[: real_maxlen])  \n",
    "    sequence = \"\".join([x if x in aa_dict else 'X' for x in sequence])\n",
    "    data = [\n",
    "    (\"protein1\", sequence),\n",
    "      ]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    with torch.no_grad():\n",
    "      results = input_embedding_net(batch_tokens, repr_layers=[num_layer], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][num_layer]\n",
    "    embedding = np.array(token_representations.squeeze(0).detach().to('cpu')).astype(np.float16)\n",
    "    embedding = embedding[:real_maxlen + 2, ]\n",
    "    embedding = embedding.mean(0) if Final_pool else embedding\n",
    "    output_dict[record.id] = embedding\n",
    "  if output_file:\n",
    "      try:\n",
    "          with open(output_file, 'wb') as f:\n",
    "            pickle.dump(output_dict, f)\n",
    "          print(f'ESM2 array saved as {output_file}')\n",
    "      except:\n",
    "          print(f'ESM2 array failed to save as {output_file}')\n",
    "          import uuid\n",
    "          tmp_name = str(uuid.uuid4())+'_esm'\n",
    "          output_file =os.path.join(os.path.dirname(input_fasta), tmp_name) \n",
    "          with open(output_file, 'wb') as f:\n",
    "            pickle.dump(output_dict, f)\n",
    "          print(f'Temp ESM2 array saved as {output_file}')\n",
    "  if Return:\n",
    "      return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hidden_layer_dimensions(data_dict):\n",
    "    hidden_layer_size = None\n",
    "    for key, value in data_dict.items():\n",
    "        if not isinstance(value, np.ndarray):\n",
    "            raise ValueError(f\"Value for key '{key}' is not a numpy array.\")\n",
    "\n",
    "        current_size = value.shape[-1]\n",
    "        if hidden_layer_size is None:\n",
    "            hidden_layer_size = current_size\n",
    "        elif hidden_layer_size != current_size:\n",
    "            return None  \n",
    "\n",
    "    return hidden_layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_from_local(feature_path, silence=False):\n",
    "    '''\n",
    "    load feature dict from local path (Using pickle.load() or torch.load())\n",
    "    the dictionary is like:\n",
    "        {\n",
    "          Protein_ID : feature_array [a 1D numpy array]\n",
    "        }\n",
    "    '''\n",
    "    # Try pickle.load() function \n",
    "    try:\n",
    "        with open(feature_path, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "        if not silence:\n",
    "            print(\"File is loaded using pickle.load()\")\n",
    "        return obj\n",
    "    except (pickle.UnpicklingError, EOFError):\n",
    "        pass\n",
    "\n",
    "    # Try torch.load() function\n",
    "    try:\n",
    "        obj = torch.load(feature_path)\n",
    "        if not silence:\n",
    "            print(\"File is loaded using torch.load()\")\n",
    "        return obj\n",
    "    except (torch.serialization.UnsupportedPackageTypeError, RuntimeError):\n",
    "        pass\n",
    "\n",
    "    print(\"Unable to load file.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_fasta_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            first_line = file.readline().strip()\n",
    "            return first_line.startswith(\">\")\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root_path():\n",
    "    try:\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except:\n",
    "        current_dir = os.getcwd()\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "    return project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Potein_rep_datasets:  \n",
    "    def __init__(self, input_path, train_range = None, test_range = None, label_tag = 'label'):\n",
    "        '''\n",
    "        [input_path] is a Path_dict containing feature ID and corresponding Local_path\n",
    "        e.g {'feature':'./path/to/you/feature_file'}\n",
    "        '''\n",
    "        sequence_data = {}\n",
    "        try:\n",
    "            for key, value in input_path.items():\n",
    "                if isinstance(value, str):\n",
    "                    print(f\"try to load feature from path:{value}\")\n",
    "                    tmp = load_feature_from_local(value)\n",
    "                elif isinstance(value, np.ndarray):\n",
    "                    print(f\"try to load feature from numpy_array\")\n",
    "                    tmp = value\n",
    "                else:\n",
    "                    print(f\"can not load feature {key}\")\n",
    "                    continue\n",
    "                for ID, feat in tmp.items():\n",
    "                    if ID not in sequence_data:\n",
    "                        sequence_data[ID] = {key:feat}\n",
    "                    else:\n",
    "                        sequence_data[ID].update( {key:feat} )\n",
    "            \n",
    "            self.sequence_data = {}   \n",
    "            for key, value in sequence_data.items():\n",
    "                if len(value) < len(input_path):\n",
    "                   print(f\"imcomplete feature ID {key} removed\")\n",
    "                else:\n",
    "                   self.sequence_data[key] = value\n",
    "            \n",
    "            if label_tag not in input_path:\n",
    "                print(f\"Add mock label [{label_tag}] of 0 for each sample\")\n",
    "                for key in self.sequence_data:\n",
    "                    self.sequence_data[key][label_tag] = 0\n",
    "        except:\n",
    "            print(f\"No valid [input_path] to load : {input_path}, return an empty dataset\")\n",
    "            self.sequence_data = {}\n",
    "               \n",
    "        self.data_indices = {i : ID for i, ID in enumerate(self.sequence_data)}\n",
    "        \n",
    "        self.label_tag = label_tag\n",
    "        \n",
    "        print(f\"total {len(self.data_indices)} sample loaded\")\n",
    "        self.feature_list = list(input_path.keys())\n",
    "        \n",
    "        self.train_range = train_range\n",
    "        self.test_range = test_range\n",
    "        \n",
    "        if not self.train_range:\n",
    "            self.train_range = range(len(self.data_indices))\n",
    "      \n",
    "        if not self.test_range:\n",
    "            self.test_range = range(len(self.data_indices))\n",
    "            \n",
    "    def __len__(self):\n",
    "        # 返回数据集的样本数量\n",
    "        return len(self.data_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ID = self.data_indices[idx]\n",
    "        sample = self.sequence_data[ID]\n",
    "        features = {key: value for key, value in sample.items() if key != self.label_tag}\n",
    "        label = sample[self.label_tag]\n",
    "        return features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNCELoss(nn.Module):\n",
    "    def __init__(self, temperature=1.0):\n",
    "        super(InfoNCELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, image_embeds, text_embeds):\n",
    "        \n",
    "        # L2归一化\n",
    "        image_embeds = F.normalize(image_embeds, dim=-1, p=2)\n",
    "        text_embeds = F.normalize(text_embeds, dim=-1, p=2)\n",
    "\n",
    "        # [batchsize, 1, embedding_dim] [1, batchsize, embedding_dim] -> [batchsize, batchsize]\n",
    "        similarity_matrix = F.cosine_similarity(image_embeds.unsqueeze(1), text_embeds.unsqueeze(0), dim=-1) / self.temperature\n",
    "\n",
    "        # 提取对角元素\n",
    "        positives = torch.diag(similarity_matrix)\n",
    "\n",
    "        nce_loss = -torch.log(torch.exp(positives) / torch.exp(similarity_matrix).sum(dim=-1)).mean()\n",
    "\n",
    "        return nce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clef(input_file_config,\n",
    "                output_dir,\n",
    "                model_initial = clef_multimodal,\n",
    "                tmp_dir = \"./tmp\",\n",
    "                embedding_generator = fasta_to_EsmRep,\n",
    "                esm_config = None,\n",
    "                train_config = None,\n",
    "                ):\n",
    "    # 初始化日志\n",
    "    log = []\n",
    "    \n",
    "    # 如果临时目录不存在，则创建它\n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.mkdir(tmp_dir)\n",
    "        \n",
    "    # input_file_config 中包含 seq 键\n",
    "    assert \"seq\" in  input_file_config , \"For training.sequence representation or fasta file\"   \n",
    "    input_file = input_file_config['seq']\n",
    "    \n",
    "    # 遍历其他键值对，检查dim形状\n",
    "    feat_dim_config = {}\n",
    "    for key, value in input_file_config.items():\n",
    "        if key != 'seq':\n",
    "            feat_dim = check_hidden_layer_dimensions(load_feature_from_local(value, silence=True))\n",
    "            assert feat_dim, f'Dimension numbers of the last dimension is not same; {value}'\n",
    "            feat_dim_config[key] = feat_dim\n",
    "            line = f'{key}--{value}; num_dims:{feat_dim}'\n",
    "            print(line)\n",
    "            log.append(f'{line}\\n')\n",
    "            \n",
    "    # 如果输入是fasta文件，则配置ESM并输出成为embedding\n",
    "    # if is_fasta_file(input_file):\n",
    "    #     print(f\"Transform representation from fasta file {input_file}\")\n",
    "    #     import uuid\n",
    "    #     tmp_file = str(uuid.uuid4())+'_tmp_esm'\n",
    "    #     tmp_file = os.path.join(tmp_dir, tmp_file)\n",
    "    #     esm_config = esm_config if isinstance(esm_config, dict) else {'Final_pool':False, 'maxlen':256}\n",
    "    #     esm_config['input_fasta'] = input_file\n",
    "    #     esm_config['output_file'] = tmp_file\n",
    "    #     esm_config['Return'] = False\n",
    "    #     if 'pretrained_model_params' not in esm_config:\n",
    "    #         esm_config['pretrained_model_params'] = os.path.join(find_root_path(), \"./pretrained_model/esm2_t33_650M_UR50D.pt\")\n",
    "    #     try:\n",
    "    #         embedding_generator(**esm_config)\n",
    "    #     except:\n",
    "    #         print(\"Failed to transform fasta into ESM embeddings, make sure esm config is correct\")\n",
    "    #         import shutil\n",
    "    #         shutil.rmtree(tmp_dir)\n",
    "    #         sys.exit(1)\n",
    "    # else:\n",
    "    #     print(f\"Direct use {input_file} as sequence representation\")\n",
    "    #     tmp_file = input_file\n",
    "    tmp_file = \"./tmp/15610eeb-88d7-40ba-98be-7fb6c6df25e4_tmp_esm\"\n",
    "    \n",
    "    # 训练设备\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu' \n",
    "    if device == 'cpu':\n",
    "        print(f'**Note:model will be trained on CPU, it may take very long time')\n",
    "        \n",
    "    # 加载序列嵌入数据并检查维度\n",
    "    num_embeds = check_hidden_layer_dimensions(load_feature_from_local(tmp_file, silence=True))\n",
    "    assert num_embeds, f'make sure {tmp_file} is a dict with ID:sequence_reps; sequence_reps should be 2D numpy with shape of [protein_length, num_hidden]'\n",
    "    line = f'Sequnce data--{input_file}; num_dims:{num_embeds}'\n",
    "    log.append(f'{line}\\n')\n",
    "    \n",
    "    # 初始化模型\n",
    "    model=model_initial(num_embeds, feat_dim_config).to(device)\n",
    "    \n",
    "    # 配置数据路径\n",
    "    data_path_config = {'esm_feature':tmp_file}\n",
    "    data_path_config.update({key:value for key, value in input_file_config.items() if key != 'seq'})\n",
    "    Dataset = Potein_rep_datasets(data_path_config)\n",
    "    \n",
    "    \n",
    "\n",
    "    # 如果数据集为空，抛出异常并删除临时目录，然后退出程序\n",
    "    # if len(Dataset) == 0:\n",
    "    #     raise ValueError(\"Failed to load feature for training\")\n",
    "    #     import shutil\n",
    "    #     shutil.rmtree(tmp_dir)\n",
    "    #     sys.exit(1)\n",
    "    \n",
    "    # 定义损失函数\n",
    "    loss_function= InfoNCELoss()\n",
    "    \n",
    "    # 传入超参数，或者使用默认值\n",
    "    if train_config:\n",
    "        for x in ['lr', 'batch_size', 'num_epoch']:\n",
    "            assert x in  train_config, \"'lr', 'batch_size' and 'num_epoch' are needed when not using default train configuration \"\n",
    "        lr = train_config['lr']\n",
    "        batch_size = train_config['batch_size']\n",
    "        num_epoch = train_config['num_epoch']\n",
    "        maxlen = 256 if 'maxlen' not in train_config else train_config['maxlen']\n",
    "    else:\n",
    "        lr = 0.00002\n",
    "        batch_size = 128\n",
    "        num_epoch = 20\n",
    "        maxlen = 256\n",
    "        \n",
    "    dataloader = DataLoader(dataset=Dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    # 优化器\n",
    "    optimizer=optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    # 不使用激活函数\n",
    "    activation = None\n",
    "    \n",
    "    # 模型临时保存点\n",
    "    temp_check_point = output_dir\n",
    "    \n",
    "    # 不跟随epoch 保存 \n",
    "    save_by_epoch = False\n",
    "    check_list = []\n",
    "    \n",
    "    # 创建检查点目录\n",
    "    if not os.path.exists(temp_check_point):\n",
    "        os.mkdir(temp_check_point)\n",
    "        \n",
    "    # 训练开始时间\n",
    "    s = time.time()\n",
    "    \n",
    "    # epoch循环\n",
    "    for epoch in range(num_epoch):\n",
    "        Loss=0\n",
    "        sum=0 \n",
    "        print(\"Start training: \"+str(epoch)+\"\\n\")\n",
    "        \n",
    "        # batch循环，Dataloader加载数据\n",
    "        # for batch in Dataset.Dataloader(batch_size=batch_size,  max_num_padding=maxlen, device=device):\n",
    "        for batch in dataloader:    \n",
    "            # eval\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                _, proj, Y = model(batch)\n",
    "                loss = loss_function(proj, Y)\n",
    "                # 损失不是NaN\n",
    "                assert not np.isnan(loss.item()), 'odd loss value appears'\n",
    "                Loss += loss.item()\n",
    "                sum += Y.shape[0]\n",
    "                \n",
    "            # train\n",
    "            model.train()\n",
    "            _, proj, Y = model(batch)\n",
    "            loss = loss_function(proj,Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # epoch训练结束时间，平均损失\n",
    "        e = time.time()\n",
    "        t = e - s\n",
    "        avg_loss = Loss / sum\n",
    "        line = f'Epoch: {epoch}; Train loss:{avg_loss}; time:{t} s'\n",
    "        print(line)\n",
    "        log.append(f'{line}\\n')\n",
    "        \n",
    "        # 保存checkpoint\n",
    "        tmp_check_path = os.path.join(temp_check_point, f\"./checkpoint_{epoch}.pt\")\n",
    "        if save_by_epoch:\n",
    "            torch.save(model.state_dict(), tmp_check_path)\n",
    "        elif epoch == num_epoch - 1 or epoch + 1 in check_list:\n",
    "            torch.save(model.state_dict(), tmp_check_path)\n",
    "            line = f'Epoch:{epoch} Checkpoint weights saved--{tmp_check_path}'\n",
    "            print(line)\n",
    "            log.append(f'{line}\\n')\n",
    "    \n",
    "    # 训练结束时间\n",
    "    e = time.time()\n",
    "    print(f'Total training time : {e-s} seconds')\n",
    "    print(f\"Done..\")\n",
    "    \n",
    "    \n",
    "    import shutil \n",
    "    try:\n",
    "        shutil.rmtree(tmp_dir)\n",
    "    except:\n",
    "        print(f\"Failed to remove temp file in {tmp_dir}.\")\n",
    "    log_path = os.path.join(output_dir, 'log.txt')\n",
    "    with open(log_path, 'w') as f:\n",
    "        f.writelines(log)\n",
    "    print(f\"Log text file saved to {log_path}.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clef(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "modal_0--./Demo_train/Demo_trainset_featA; num_dims:1024\n",
    "\n",
    "modal_1--./Demo_train/Demo_trainset_featB; num_dims:768\n",
    "\n",
    "modal_2--./Demo_train/Demo_trainset_featC; num_dims:256\n",
    "\n",
    "Transform representation from fasta file ./Demo_train/Demo_trainset.faa\n",
    "\n",
    "Skip loading local pre-trained ESM2 model from pretrained_model\\esm2_t33_650M_UR50D.pt.\n",
    "\n",
    "Try to use ESM2-650M downloaded from hub\n",
    "\n",
    "ESM2 array saved as ./tmp\\aeaae50d-41b3-476b-b6cf-fc685a7cc631_tmp_esm\n",
    "\n",
    "try to load feature from path:./tmp\\aeaae50d-41b3-476b-b6cf-fc685a7cc631_tmp_esm\n",
    "\n",
    "File is loaded using pickle.load()\n",
    "\n",
    "try to load feature from path:./Demo_train/Demo_trainset_featA\n",
    "\n",
    "File is loaded using pickle.load()\n",
    "\n",
    "try to load feature from path:./Demo_train/Demo_trainset_featB\n",
    "\n",
    "File is loaded using pickle.load()\n",
    "\n",
    "try to load feature from path:./Demo_train/Demo_trainset_featC\n",
    "\n",
    "File is loaded using pickle.load()\n",
    "\n",
    "Add mock label [label] of 0 for each sample\n",
    "\n",
    "Add mock label [label] of 0 for each sample\n",
    "\n",
    "Add mock label [label] of 0 for each sample\n",
    "\n",
    "Add mock label [label] of 0 for each sample\n",
    "\n",
    "Add mock label [label] of 0 for each sample\n",
    "\n",
    "Add mock label [label] of 0 for each sample\n",
    "\n",
    "total 3178 sample loaded\n",
    "\n",
    "total 3178 sample loaded\n",
    "\n",
    "Epoch: 0; Train loss:0.06013690780137904; time:222.6505126953125 s\n",
    "\n",
    "Epoch: 1; Train loss:0.056203456927276095; time:449.27208948135376 s\n",
    "\n",
    "Epoch: 2; Train loss:0.054905163334029665; time:673.4195473194122 s\n",
    "\n",
    "Epoch: 3; Train loss:0.05423521492899852; time:892.0980730056763 s\n",
    "\n",
    "Epoch: 4; Train loss:0.05371454446251247; time:1112.6760714054108 s\n",
    "\n",
    "Epoch: 5; Train loss:0.0534589481173708; time:1329.012489080429 s\n",
    "\n",
    "Epoch: 6; Train loss:0.0532391931817095; time:1548.2551367282867 s\n",
    "\n",
    "Epoch: 7; Train loss:0.05299747657895913; time:1764.3475527763367 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "\n",
    "Epoch: 6; Train loss:0.0532391931817095; time:1548.2551367282867 s\n",
    "\n",
    "Epoch: 7; Train loss:0.05299747657895913; time:1764.3475527763367 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 6; Train loss:0.0532391931817095; time:1548.2551367282867 s\n",
    "\n",
    "Epoch: 7; Train loss:0.05299747657895913; time:1764.3475527763367 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 6; Train loss:0.0532391931817095; time:1548.2551367282867 s\n",
    "\n",
    "Epoch: 7; Train loss:0.05299747657895913; time:1764.3475527763367 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 6; Train loss:0.0532391931817095; time:1548.2551367282867 s\n",
    "\n",
    "Epoch: 7; Train loss:0.05299747657895913; time:1764.3475527763367 s\n",
    "\n",
    "Epoch: 6; Train loss:0.0532391931817095; time:1548.2551367282867 s\n",
    "\n",
    "Epoch: 6; Train loss:0.0532391931817095; time:1548.2551367282867 s\n",
    "\n",
    "Epoch: 7; Train loss:0.05299747657895913; time:1764.3475527763367 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 10; Train loss:0.052566751809597315; time:3605.0055277347565 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 10; Train loss:0.052566751809597315; time:3605.0055277347565 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 8; Train loss:0.05278185840820351; time:1979.615389585495 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 10; Train loss:0.052566751809597315; time:3605.0055277347565 s\n",
    "\n",
    "Epoch: 11; Train loss:0.0525134675078155; time:3822.644381761551 s\n",
    "\n",
    "Epoch: 12; Train loss:0.05236718435569557; time:4044.220722913742 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 10; Train loss:0.052566751809597315; time:3605.0055277347565 s\n",
    "\n",
    "Epoch: 11; Train loss:0.0525134675078155; time:3822.644381761551 s\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 10; Train loss:0.052566751809597315; time:3605.0055277347565 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 9; Train loss:0.0526635484773457; time:3390.4365899562836 s\n",
    "\n",
    "Epoch: 10; Train loss:0.052566751809597315; time:3605.0055277347565 s\n",
    "\n",
    "Epoch: 11; Train loss:0.0525134675078155; time:3822.644381761551 s\n",
    "\n",
    "Epoch: 11; Train loss:0.0525134675078155; time:3822.644381761551 s\n",
    "\n",
    "Epoch: 12; Train loss:0.05236718435569557; time:4044.220722913742 s\n",
    "\n",
    "Epoch: 13; Train loss:0.052220565331964994; time:4268.119744300842 s\n",
    "\n",
    "Epoch: 13; Train loss:0.052220565331964994; time:4268.119744300842 s\n",
    "\n",
    "Epoch: 14; Train loss:0.05217440335385975; time:4483.992960929871 s\n",
    "\n",
    "Epoch: 14; Train loss:0.05217440335385975; time:4483.992960929871 s\n",
    "\n",
    "Epoch: 15; Train loss:0.05205994315534511; time:4700.0446581840515 s\n",
    "\n",
    "Epoch: 16; Train loss:0.05203925685510461; time:4916.996402025223 s\n",
    "\n",
    "Epoch: 16; Train loss:0.05203925685510461; time:4916.996402025223 s\n",
    "\n",
    "Epoch: 17; Train loss:0.05193668345072646; time:5133.026588916779 s\n",
    "\n",
    "Epoch: 17; Train loss:0.05193668345072646; time:5133.026588916779 s\n",
    "\n",
    "Epoch: 18; Train loss:0.05185567987572854; time:5349.241844177246 s\n",
    "\n",
    "Epoch: 19; Train loss:0.051844323034028424; time:5565.7364683151245 s\n",
    "\n",
    "Epoch:19 Checkpoint weights saved--Demo_clef\\./checkpoint_19.pt\n",
    "\n",
    "Total training time : 5567.153565168381 seconds\n",
    "\n",
    "Done..\n",
    "\n",
    "Log text file saved to Demo_clef\\log.txt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
